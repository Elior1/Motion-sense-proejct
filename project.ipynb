{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None coding part:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. ROC is a popular graphic simultaneously displaying the two types of errors (FPR, TPR) for all possible thresholds. The overall performance of a classifier, summarized\n",
    "over all possible thresholds, is given by the area under the curve.\n",
    "The advantage of this metric is that it considers all possible thresholds, in that way we can compare two classifiers without the need to choose a specific threshold.\n",
    "while evaluating the overall performance is important, at some areas, high AUC is not interpretable. ROC AUC treats sensitivity and specificity as equally important overall when averaged across all thresholds, but what if we care more about the sensitivity, meaning correctly predicting a cancer and treating it? in that situation we would prefer to look at a specific measure and not the total performance of a classifier. \n",
    "\n",
    "B. Accuracy, as the name states, measures how accurate is our classifier, meaning how much did we predicted correctly over all predictions. It is a good metric when we have a balanced dataset and we don't care to be more accurate in out prediction to one class then the other. On the other hand, when we have an unbalanced dataset, where we have for example 95 observation of class 0 and 5 observations of class 1, accuracy preforms poorly.\n",
    "If we would decide to predict all the observation to be 0, we would get 95% accuracy and we would think we have a good classifier while we didn't predict correctly the other class which is probably more interesting to predict.\n",
    "\n",
    "C. F1 considers both Precision and Recall, so unlike Accuracy, F1 performs well with unbalanced data and for the example above F1 will be 0. Its disadvantage is that it gives equal importance to both Precision and Recall. For example, if we what we care the most is that our classifier's positive predictions are truly positive, then we would want to the precision to have more weight than the Recall and F1 might indicate that our classifier performs poorly while it performs well.\n",
    "\n",
    "D. Log loss measures the performance of a classification model where the prediction input is a probability value between 0 and 1. We aim to minimize its value as possible. The perfect model has a value of 0. It used mainly for comparing between different models, but its value doesn't tell us how our model is preforming in terms of accurate prediction in each class.\n",
    "\n",
    "E. Splitting the data to train and test is very important when evaluating a classifier. That way we try to estimate the error rate of our model in the real world. \n",
    "High differences between train performance values and test performance values, meaning low error rate in train set but high error rate in test set, may indicate that our model is overfitting and we should retune our model, or try different resampling method.\n",
    "F.  MCC have an advantage over Accuracy and F1 by that it is not sensitive to which class we assign as positive and negative. For example, lets say we have TP =95, FP = 5, TN = 0, FN = 0. In this situation we will have Accuracy = 95% and F1 = 97% while MCC is not defined so we will notice that our model is going in the wrong direction. \n",
    "G. Cohen's Kappa is a statistic which measures inter-rater agreement. It's a measure that ranges between [-1,1] and therefore it's not clear what value counts as high agreement. Furthermore, it may perform differently then other metrics when data is unbalanced.\n",
    "For example, if we have a confusion matrix with TP = 0, TN = 14, FP = 1, FN = 1 then we get accuracy of 0.875 and kappa value of -0.066. So, it's important to look not only at the value of a matric but also at the quantities at each predicted class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the file includes data subject information.\n",
    "\n",
    "    Data Columns:\n",
    "    0: code [1-24]\n",
    "    1: weight [kg]\n",
    "    2: height\n",
    "    3: age [years]\n",
    "    4: gender [0:Female, 1:Male]\n",
    "\n",
    "Returns:\n",
    "A pandas DataFrame that contains inforamtion about data subjects' attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "root = \"C:/Users/elior/PycharmProjects/data_wrangling/\"\n",
    "def get_ds_infos():\n",
    "    dss = pd.read_csv(root + \"data_subjects_info.csv\")\n",
    "    print(\"[INFO] -- Data subjects' information is imported.\")\n",
    "    return dss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the sensors and the mode to shape the final dataset.\n",
    "\n",
    "Args:\n",
    "data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate,\n",
    "userAcceleration]\n",
    "\n",
    "Returns:\n",
    "It returns a list of columns to use for creating time-series from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data_types(data_types=[\"userAcceleration\"]):\n",
    "    dt_list = []\n",
    "    for t in data_types:\n",
    "        if t != \"attitude\":\n",
    "            dt_list.append([t + \".x\", t + \".y\", t + \".z\"])\n",
    "        else:\n",
    "            dt_list.append([t + \".roll\", t + \".pitch\", t + \".yaw\"])\n",
    "\n",
    "    return dt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Args:\n",
    "    dt_list: A list of columns that shows the type of data we want.\n",
    "    act_labels: list of activites\n",
    "    trial_codes: list of trials\n",
    "    mode: It can be \"raw\" which means you want raw data\n",
    "    for every dimention of each data type,\n",
    "    [attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)].\n",
    "    or it can be \"mag\" which means you only want the magnitude for each data type: \n",
    "    (x^2+y^2+z^2)^(1/2)\n",
    "    labeled: True, if we want a labeld dataset. False, if we only want sensor values.\n",
    "Returns:\n",
    "        It returns a time-series of sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True):\n",
    "\n",
    "    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list * 3)\n",
    "\n",
    "    if labeled:\n",
    "        dataset = np.zeros((0, num_data_cols + 7))  # \"7\" --> [act, code, weight, height, age, gender, trial]\n",
    "    else:\n",
    "        dataset = np.zeros((0, num_data_cols))\n",
    "\n",
    "    ds_list = get_ds_infos()\n",
    "\n",
    "    print(\"[INFO] -- Creating Time-Series\")\n",
    "    for sub_id in ds_list[\"code\"]:\n",
    "        for act_id, act in enumerate(act_labels):\n",
    "            for trial in trial_codes[act_id]:\n",
    "                fname = root + 'A_DeviceMotion_data/' + act + '_' + str(trial) + '/sub_' + str(int(sub_id)) + '.csv'\n",
    "                raw_data = pd.read_csv(fname)\n",
    "                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n",
    "                vals = np.zeros((len(raw_data), num_data_cols))\n",
    "                for x_id, axes in enumerate(dt_list):\n",
    "                    if mode == \"mag\":\n",
    "                        vals[:, x_id] = (raw_data[axes] ** 2).sum(axis=1) ** 0.5\n",
    "                    else:\n",
    "                        vals[:, x_id * 3:(x_id + 1) * 3] = raw_data[axes].values\n",
    "                    vals = vals[:, :num_data_cols]\n",
    "                if labeled:\n",
    "                    lbls = np.array([[act_id,\n",
    "                                      sub_id - 1,\n",
    "                                      ds_list[\"weight\"][sub_id - 1],\n",
    "                                      ds_list[\"height\"][sub_id - 1],\n",
    "                                      ds_list[\"age\"][sub_id - 1],\n",
    "                                      ds_list[\"gender\"][sub_id - 1],\n",
    "\n",
    "                                      trial\n",
    "                                      ]] * len(raw_data))\n",
    "                    vals = np.concatenate((vals, lbls), axis=1)\n",
    "                dataset = np.append(dataset, vals, axis=0)\n",
    "    cols = []\n",
    "    for axes in dt_list:\n",
    "        if mode == \"raw\":\n",
    "            cols += axes\n",
    "        else:\n",
    "            cols += [str(axes[0][:-2])]\n",
    "\n",
    "    if labeled:\n",
    "        cols += [\"act\", \"id\", \"weight\", \"height\", \"age\", \"gender\", \"trial\"]\n",
    "\n",
    "    dataset = pd.DataFrame(data=dataset, columns=cols)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating The Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT_LABELS = [\"dws\", \"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n",
    "TRIAL_CODES = {\n",
    "    ACT_LABELS[0]: [1, 2, 11],\n",
    "    ACT_LABELS[1]: [3, 4, 12],\n",
    "    ACT_LABELS[2]: [7, 8, 15],\n",
    "    ACT_LABELS[3]: [9, 16],\n",
    "    ACT_LABELS[4]: [6, 14],\n",
    "    ACT_LABELS[5]: [5, 13]\n",
    "}\n",
    "\n",
    "# Here we set parameter to build labeled time-series from data set of \"(A)DeviceMotion_data\"\n",
    "# attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\n",
    "sdt = [\"attitude\", \"gravity\", \"rotationRate\", \"userAcceleration\"]\n",
    "print(\"[INFO] -- Selected sensor data types: \" + str(sdt))\n",
    "act_labels = ACT_LABELS\n",
    "print(\"[INFO] -- Selected activites: \" + str(act_labels))\n",
    "trial_codes = [TRIAL_CODES[act] for act in act_labels]\n",
    "dt_list = set_data_types(sdt)\n",
    "dataset = creat_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True)\n",
    "print(\"[INFO] -- Shape of time-Series dataset:\" + str(dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting to train and test\n",
    "All trials from 1-9 are for training and 11-16 are fr testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame()\n",
    "x_test = pd.DataFrame()\n",
    "y_train = pd.DataFrame()\n",
    "x_train = pd.DataFrame()\n",
    "\n",
    "for j in set(dataset['act']):\n",
    "    for i in set(dataset[dataset['act'] == j]['trial']):\n",
    "        if i > 10:\n",
    "            y_test = pd.concat([y_test, dataset[dataset['act'] == j][dataset[dataset['act'] ==\n",
    "                                                                             j]['trial'] == i]['act']])\n",
    "            x_test = pd.concat([x_test, dataset[dataset['act'] == j][dataset[dataset['act'] == j]['trial'] ==\n",
    "                                         i][dataset.columns[dataset.columns != 'act']]])\n",
    "        else:\n",
    "            y_train = pd.concat([y_train, dataset[dataset['act'] == j][dataset[dataset['act'] ==\n",
    "                                                                              j]['trial'] == i]['act']])\n",
    "            x_train = pd.concat([x_train, dataset[dataset['act'] == j][dataset[dataset['act'] == j]['trial'] ==\n",
    "                                              i][dataset.columns[dataset.columns != 'act']]])\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Long Short Term Memory neural network to predict the next activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123) # for reproducibility\n",
    "batch_size = 50000\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(1081446, 10))\n",
    "model.add(keras.layers.LSTM(128, dropout=0.7))\n",
    "model.add(keras.layers.Dense(6))\n",
    "model.add(keras.layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train\n",
    "np.random.seed(123) # for reproducibility\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=5,verbose=0,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "train_score, train_acc = model.evaluate(x_train, y_train, batch_size=batch_size)\n",
    "test_score, test_acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Train score:', train_score)\n",
    "print('Train accuracy:', train_acc)\n",
    "print('Test score:', test_score)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy: 0.8112712  \n",
    "Test accuracy: 0.45008284  \n",
    "\n",
    "It's obvious that my model is overfitting, hence the big difference between train accuracy and test accuracy.\n",
    "I belive that if I had more computational power for retuning the parameters of the model I would be able to get better results. Moreover, If I had more time to spend in research about this type of classification problems and the appropriate algorithms and methods that should be used I couldget better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
